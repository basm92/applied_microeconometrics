---
title: "Applied Microeconometrics - Assignment 3"
author: "Walter Verwer (589962) & Bas Machielsen (590049)"
date: \today
output:
  pdf_document:
    includes:
      in_header: preamble.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse); library(plm); library(stargazer); library(modelsummary)
library(kableExtra); library(MatchIt); library(plm); library(sandwich)

dataset <- haven::read_dta("./CardKrueger.dta") 
```

Construct a variable full-time equivalent for both waves, which is the number of full-time employees plus the number of part-time employees divided by two and also add the number of managers.  I will simply refer to employees for this outcome variable. 

```{r}
dataset <- dataset %>%
    mutate(employees = EMPFT  + EMPPT/2 + NMGRS,
           employees2 = EMPFT2 + EMPPT2/2 + NMGRS2,
           changeemployees = employees - employees2)

```

(i) Compute separately for New Jersey and Pennsylvania the average number of employees in both waves, and compute the difference-in-difference estimate

```{r}
did <- dataset %>%
    group_by(STATE) %>%
    summarize(mean_before = mean(employees, na.rm = TRUE),
              mean_after = mean(employees2, na.rm = TRUE)) %>%
    mutate(STATE = if_else(STATE == 0, "PA", "NJ")) 

did %>%
  knitr::kable(booktabs=T) %>%
  kableExtra::kable_styling(font_size = 7, latex_options = "hold_position")

did$mean_after[2] - did$mean_after[1] - (did$mean_before[2] - did$mean_before[1])
```

Next  repeat  this,  but  only  considering  the  restaurants  that  responded  in both waves of the survey.

```{r}
did2 <- dataset %>%
    group_by(STATE) %>%
    filter(!is.na(employees), !is.na(employees2)) %>%
    summarize(mean_before = mean(employees, na.rm = TRUE),
              mean_after = mean(employees2, na.rm = TRUE)) %>%
    mutate(STATE = if_else(STATE == 0, "PA", "NJ")) 
    
did2$mean_after[2] - did2$mean_after[1] - (did2$mean_before[2] - did2$mean_before[1])
```

\clearpage

(ii) Estimate this model and next subsequently add characteristics of the restaurants observed in the first wave.  But think carefully which characteristics can be included.  How does the latter affect the estimate for the coefficient $\delta$?

```{r, results='asis'}

model1 <- lm(data = dataset, 
   formula = changeemployees ~ STATE)
model2 <- lm(data = dataset, 
             formula = changeemployees ~ STATE + SOUTHJ + CENTRALJ + SHORE + PA1)
model3 <- update(model1, . ~ . + NCALLS + WAGE_ST + INCTIME + FIRSTINC +
                   BONUS + MEALS + OPEN + HRSOPEN + PSODA + PFRY + NREGS + NREGS11)
model4 <- update(model2, . ~ . + NCALLS + WAGE_ST + INCTIME + FIRSTINC +
                   BONUS + MEALS + OPEN + HRSOPEN + PSODA + PFRY + NREGS + NREGS11)

models <- list(model1, model2, model3, model4) 

# Adjust standard errors
cov1 <- vcovHC(model1, type = "HC1")
robust_se1    <- sqrt(diag(cov1))

cov2 <- vcovHC(model2, type = "HC1")
robust_se2    <- sqrt(diag(cov2))

cov3 <- vcovHC(model3, type = "HC1")
robust_se3    <- sqrt(diag(cov3))

cov4 <- vcovHC(model4, type = "HC1")
robust_se4    <- sqrt(diag(cov4))

stargazer(models, omit.stat = c("ll", "ser", "rsq"), df=F, 
          omit = c("SOUTHJ", "CENTRALJ", "SHORE", "PA1"),
          add.lines = list(c("Region Dummies", "No", "Yes", "No", "Yes")),
          header = FALSE, font.size = "footnotesize", title='Estimated model for Q2 with robust standard errors.',
          se=list(robust_se1, robust_se2, robust_se3,
                  robust_se4))

```

We want to isolate the effect of the minimum wage by attributing it to the coefficient belonging to STATE, which means we have to account for all possible sources of variation not due to the minimum wage. This also means we cannot control for PCTAFF, because this is the mechanism we care about: if we conditioned on this variable, that would absorb all variation due to the minimum wage policy changes and would change our interpretation of the STATE coefficient to a partial instead of a total effect and bias it towards zero. 

\clearpage 

(iii) Provide  a  balancing  table,  i.e.   show  the  sample  mean  of  characteristics observed  in  the  first  survey  separately  for  the  restaurants  in  New  Jersey and Pennsylvania.  What is your opinion about the balancing table?

We think that the table indicates that the covariates are balanced across the two groups, approaching the situation of an experiment. There are, however, a couple of variables that are significantly different across the two groups: MEALS, PSODA and PFRY. Omitting them could have consequences for the estimated effect of the minimum wage on employment, because those variables (i.e. MEALS, PSODA and FRY) could be correlated or even have a causal effect on employment. For example, if the price is higher in one country, they might serve a higher segment of the market, and higher less personnel in general, and pay above the minimum wage. 


```{r, results='asis', warning = FALSE}
dataset %>%
    mutate(STATE = if_else(STATE == 0, "PA", "NJ")) %>%
    filter(!is.na(changeemployees)) %>%
    rename_with(.fn = ~ stringr::str_replace(.x, "_", "")) %>%
    select(changeemployees, STATE, NCALLS, WAGEST, INCTIME, FIRSTINC, BONUS, PCTAFF, MEALS,
           OPEN, HRSOPEN, PSODA, PFRY, PENTREE, NREGS, NREGS11) %>%
    modelsummary::datasummary_balance(formula = ~ STATE, 
                                      dinm= TRUE, 
                                      output = "latex", 
                                      fmt = "%.3f",
                                      dinm_statistic = "p.value") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

None of this is particularly plausible in our opinion, but it makes sense to control for these factors anyway, so that the eventual effect takes into account these possible causal links. 

\clearpage

(iv) Check  for  the  different  characteristics  if  there  is  a  common  support  for restaurants in New Jersey and Pennsylvania.  And estimate a propensity score for being a restaurant in New Jersey.

```{r, warning = FALSE}
# Check for common support
dataset2 <- dataset %>%
    mutate(STATE = if_else(STATE == 0, "PA", "NJ")) %>%
    filter(!is.na(changeemployees)) %>%
    rename_with(.fn = ~ stringr::str_replace(.x, "_", "")) %>%
    select(changeemployees, STATE, NCALLS, WAGEST, INCTIME, FIRSTINC, BONUS, PCTAFF, MEALS,
           OPEN, HRSOPEN, PSODA, PFRY, PENTREE, NREGS, NREGS11) 

emptycol = function(x) " "

boxplot1 <- lapply(dataset2 %>%
                     filter(STATE == "NJ") %>%
                  select(-STATE), na.omit) %>% lapply(scale)
boxplot2 <- lapply(dataset2 %>%
                     filter(STATE == "PA") %>%
                  select(-STATE), na.omit) %>% lapply(scale)

modelsummary::datasummary(
  data = dataset2, 
   NCALLS + WAGEST + INCTIME + FIRSTINC + BONUS + PCTAFF + MEALS +
           OPEN + HRSOPEN + PSODA + PFRY + PENTREE + NREGS + NREGS11 ~ 
    STATE * (Mean + SD + Heading("Boxplot")*emptycol + Heading("Histogram")*emptycol),
  title = 'Common support table.') %>%
  column_spec(column = 4, image = spec_boxplot(boxplot1)) %>%
  column_spec(column = 8, image = spec_boxplot(boxplot2)) %>%
  column_spec(column = 5, image = spec_hist(boxplot1)) %>%
  column_spec(column = 9, image = spec_hist(boxplot2)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")


```

As indicated in the table, there is no common support for any of the variables, as we are dealing with continuous variables, so that the probability of realizing two zero outcomes is practically zero. We estimate two propensity scores, one extensive model, which sacrifices many observations, and one parsimonious model, which does not. 

```{r}
# Estimate propensity score
ps1 <- glm(STATE ~ WAGE_ST + INCTIME + FIRSTINC + BONUS + PCTAFF + MEALS + OPEN + HRSOPEN +
      PSODA + PFRY + PENTREE + NREGS + NREGS11 + NCALLS, 
    data = dataset, 
    family="binomial") 

ps2 <- glm(STATE ~ MEALS + OPEN + HRSOPEN + PSODA + PFRY, 
           data = dataset, 
    family="binomial") 

dataset <- dataset %>%
  modelr::add_predictions(ps1, type = "response") %>%
  rename("ps1" = "pred")

dataset <- dataset %>%
  modelr::add_predictions(ps2, type = "response") %>%
  rename("p2" = "pred")
```

We show the distribution of propensity scores across the two groups:

```{r, warning = FALSE, message = FALSE}
dataset %>%
  ggplot(aes(x = ps1)) + geom_histogram()+ facet_wrap(~as.factor(STATE)) + theme_bw()
```

\clearpage

(v) Use propensity score matching to estimate the average treatment effect on the treated for the employment before and after the minimum wage increase in New Jersey, so on $E_{0i}$ and $E_{1i}$ separately. 

We report the results of (v) and (vi) in table \ref{tab:hoi}. We use the `MatchIt` package to estimate the propensity-score again and subsequently match using the nearest neighbor algorithm to compute $E_{0i}$:

```{r, warning = FALSE}
matched_data1 <- MatchIt::matchit(STATE ~ MEALS + OPEN + HRSOPEN + PSODA + PFRY,
                 data = dataset %>%
                   select(employees, STATE, MEALS, OPEN, HRSOPEN, PSODA, PFRY) %>%
                   na.omit(), 
                 method = "nearest") %>%
  match.data()

e_0i <- lm(employees ~ STATE + MEALS + OPEN + HRSOPEN + PSODA + PFRY, data = matched_data1)

# Adjust standard errors
cov0i         <- vcovHC(e_0i, type = "HC1")
robust_se0i    <- sqrt(diag(cov0i))

```

And for $E_{1i}$:

```{r, warning = FALSE}
matched_data2 <- MatchIt::matchit(STATE ~ MEALS + OPEN + HRSOPEN + PSODA + PFRY,
                 data = dataset %>%
                   select(employees2, STATE, MEALS, OPEN, HRSOPEN, PSODA, PFRY) %>%
                   na.omit(), 
                 method = "nearest") %>%
  match.data()

e_1i <- lm(employees2 ~ STATE + MEALS + OPEN + HRSOPEN + PSODA + PFRY, data = matched_data2)

# Adjust standard errors
cov1i         <- vcovHC(e_1i, type = "HC1")
robust_se1i    <- sqrt(diag(cov1i))

```

\clearpage 

(vi) Now use propensity score matching to estimate the average treatment effect on the treated on the change in employment in the restaurants, so $E_{1i}-E_{0i}$. 

```{r, warning = FALSE}
matched_data3 <- MatchIt::matchit(STATE ~ MEALS + OPEN + HRSOPEN + PSODA + PFRY,
                 data = dataset %>%
                   select(changeemployees, STATE, MEALS, OPEN, HRSOPEN, PSODA, PFRY) %>%
                   na.omit(), 
                 method = "nearest") %>%
  match.data()

ate <- lm(changeemployees ~ STATE + MEALS + OPEN + HRSOPEN + PSODA + PFRY, data = matched_data3)

# Adjust standard errors
cov_ate         <- vcovHC(ate, type = "HC1")
robust_se_ate    <- sqrt(diag(cov_ate))


```


```{r, results='asis'}
stargazer(e_0i, e_1i, ate, header = F, 
          omit.stat = c("ll", "ser", "rsq"), df = F,
          font.size = "footnotesize",
          label="tab:hoi",
          title='Application of propensity score matching to estimate the average treatment 
          effect on the treated on the change in employments in the restaurants. Standard erros are robust',
          se= list(robust_se0i, robust_se1i, robust_se_ate))

```

\clearpage 

(vii) Now check the sensitivity of the propensity score matching estimate by also computing the weighting estimators for the average treatment effect on the treated. 

For transparency, we show (the first rows of) intermediate datasets we use. We first calculate the nearest neighbour weighting estimate, using the `matched_data3` data.frame, which implements nearest-neighbor matching:

```{r}
matched_data3 %>%
  arrange(subclass) %>%
  head(6) %>%
  kable(booktabs = TRUE, caption = "Nearest neigbour
                            weighting estimate.") %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
treated_subjects <- sum(matched_data3$STATE)

differences <- matched_data3 %>%
  group_split(subclass) %>%
  map_dbl(.f =~ .x$changeemployees[2]- .x$changeemployees[1])

paste('The mean difference in employees is', mean(differences))
paste('The standard deviation is', sd(differences))
```

Now, we calculate the neighborhood matching weighting estimate, with $k=4$, implemented again in the `MatchIt` package:

```{r}
matched_data4 <- MatchIt::matchit(STATE ~ MEALS + OPEN + HRSOPEN + PSODA + PFRY,
                 data = dataset %>%
                   select(changeemployees, STATE, MEALS, OPEN, HRSOPEN, PSODA, PFRY) %>%
                   na.omit(), 
                 method = "nearest", 
                 replace = TRUE,
                 ratio = 4,
                 min.controls = 3) %>%
  get_matches()

matched_data4 %>%
  arrange(subclass) %>%
  head(6) %>%
  kable(booktabs=T, caption = "Neigborhood
                            weighting estimate, with $k=4$.") %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down"))

differences <- matched_data4 %>%
  group_split(subclass) %>%
  map_dbl(.f = ~ .x$changeemployees[1] - 0.25*(.x$changeemployees[2] + 
                                                 .x$changeemployees[3] +
                                                 .x$changeemployees[4] +
                                                 .x$changeemployees[5])
  )

paste('The mean difference in employees is', mean(differences))
paste('The standard deviation is', sd(differences))
  
```

Finally, we manually implement the normal kernel density with $\Sigma = I$:

```{r}
library(mvtnorm)

normalkernel <- function(dataset){
  
  treated_outcomes <- dataset %>%
    filter(STATE ==1) %>%
    select(changeemployees)
  
  untreated_outcomes <- dataset %>%
    filter(STATE == 0) %>%
    select(changeemployees)
    
  treated_obs <- dataset %>%
    filter(STATE == 1) %>%
    select(MEALS, OPEN, HRSOPEN, PSODA, PFRY)
  
  untreated_obs <- dataset %>%
    filter(STATE == 0) %>%
    select(MEALS, OPEN, HRSOPEN, PSODA, PFRY)
  
  outcomes <- vector(length = nrow(treated_obs))
  w <- matrix(nrow = nrow(treated_obs), ncol = nrow(untreated_obs))
  
  for (i in 1:nrow(treated_obs)){
    
    for(j in 1:nrow(untreated_obs)){
      # Create the weight matrix
      w[i,j] <- mvtnorm::dmvnorm(as.numeric(untreated_obs[j,]), 
                                 mean = as.numeric(treated_obs[i,]))
    }
    
    # Normalize the weights
  weightstotal <- sum(w[i,], na.rm = TRUE)
  w[i,] = w[i,] / weightstotal
  
  # Compute the estimated outcomes
  outcomes[i] <- treated_outcomes[i,] - sum(w[i,]*untreated_outcomes[,1], na.rm = TRUE)

  }
  
  outcomes <- unlist(outcomes)
  
  print(paste('The estimated ATT is equal to:', mean(outcomes, na.rm = TRUE)))
  print(paste('The std. deviation is equal to:', sd(outcomes, na.rm = TRUE)))
  # Compute the average - final number
}

```

```{r}
normalkernel(dataset)
```