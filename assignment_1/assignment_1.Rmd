---
title: "Applied Microeconometrics - Assignment 1"
author: "Walter Verwer & Bas Machielsen"
date: "8/31/2021"
output:
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse); library(plm); library(stargazer)

dataset <- readr::read_csv("datadynpan2021.csv")
```

1. Explain why first differencing the equation does not solve the endogeneity problem of lagged consumption.

The first difference specification is: 

\begin{align*}
(\log C_{it} - \log C_{it-1}) = \beta_1 \cdot (\log p_{it} - \log p_{it-1}) + \beta_2 \cdot (\log inc_{it} - \log inc_{it-1}) + \\
\beta_3 \cdot (\log ilop_{it} - \log ilop_{it-1}) + \beta_4 + \beta_5 \cdot (\log C_{it-1} - \log C_{it-2}) + u_{it} - u_{it-1}
\end{align*}

First difference estimation is just OLS estimation with transformed data. For the OLS estimator (in general) to be consistent and unbiased, we need $\text{Cov}(X, U)=0$, where $X$ is the matrix containing all regressors. In the context of our transformed data, we need $\text{Cov}(\Delta X, \Delta u)=0$. One of the variables in $\Delta X$ is $\Delta \log C_{it-1}$. If we evaluate the covariance between $\Delta \log C_{it-1}$ and $\Delta U_{it}$, we find that:

\begin{align*}
&\text{Cov}(\Delta \log C_{it-1}, \Delta u_{it}) = \\
&\text{Cov}(\beta \Delta X_{it-1} + \beta_5 \Delta \log C_{it-2} + \Delta u_{it-1}, \Delta u_{it}) = \\
&\text{Cov}(\Delta u_{it-1}, \Delta u_{it}) \neq 0
\end{align*}

Since we observe that the exogeneity assumption is violated, we can conclude that first differencing the equation does not solve the endogeneity problem of lagged consumption.

2. Anderson & Hsiao propose a specific instrumental variable procedure for the model. Write down and perform the associated first stage regression. Comment on its outcomes.

We have to keep in mind that the first-stage regression contains all the exogenous regressors $X$ from the second stage regression, plus the instrument, $C_{it-2}$. Hence, the first-stage model is: 

\begin{align*}
\widehat{\log C_{it-1} - \log C_{it-2}} = \beta_1 \cdot (\log p_{it} - \log p_{it-1}) + \beta_2 \cdot (\log inc_{it} - \log inc_{it-1}) + \\
\beta_3 \cdot (\log ilop_{it} - \log ilop_{it-1}) + \beta_4 + \beta_5 \cdot (\log C_{it-2}) + u_{it-1} - u_{it-2}
\end{align*}

And the predicted values are to be used as follows in the second-stage regression:

<!---
Walter: moet het niet \log C_it-1 - \log C_it-2 zijn ipv zonder log?
-->

\begin{align*}
(\log C_{it} - \log C_{it-1}) = \beta_1 \cdot (\log p_{it} - \log p_{it-1}) + \beta_2 \cdot (\log inc_{it} - \log inc_{it-1}) + \\
\beta_3 \cdot (\log ilop_{it} - \log ilop_{it-1}) + \beta_4 + \beta_5 \cdot \widehat{(C_{it-1} - C_{it-2})} + u_{it} - u_{it-1}
\end{align*}

Using the data, we find the following first-stage regression (table \ref{tab:reg}):

```{r results='asis'}
## Create the first and second differences
dataset <- dataset %>%
    group_by(region) %>%
    mutate(across(contains("log"),
                  ~ .x - dplyr::lag(.x), .names = "l1_{.col}"),
           across(starts_with("log"),
                  ~ dplyr::lag(.x) - dplyr::lag(.x, 2), .names = "l2_{.col}"),
           level_quantity = dplyr::lag(logquantity, 2))

## Run the first-stage regression
first_stage_reg <- lm(formula = "l2_logquantity ~ l1_logprice + l1_logincome + l1_logillegal +
   level_quantity",
   data = dataset)

```

Whereas the F-statistic is acceptable (higher than 10), it is not _much_ higher than 10, leaving questions about the relevance of the instrument. Indeed, the instrument seems to be lacking statistical relevance, and thus predictive power. The coefficient on level quantity is only `r first_stage_reg$coefficients[5]` and insignificant at the 10% level. This means that consumption is $C_{it-2}$ does not predict differences $C_{it-1} - C_{it-2}$ well, meaning there is no clear relationship between absolute consumption and (near-)future increases/decreases of consumption. 

3. Estimate the specification above using the Anderson & Hsiao approach. Comment on the underlying assumptions, tabulate the results and comment on the outcomes.

```{r results='asis'}
# Use a package to estimate Anderson-Hsiao
dataset2 <- plm::pdata.frame(dataset, c("region", "year"))

anderson_hsiao <- plm(l1_logquantity ~ l1_logprice + l1_logincome + 
                          l1_logillegal + l2_logquantity | 
                          l1_logprice + l1_logincome + l1_logillegal + 
                          level_quantity,
                      data=dataset2,
                      model="pooling"
          )

# Compare with Manual 2SLS
dataset <- modelr::add_predictions(dataset, first_stage_reg) %>%
       rename("c_instrumented"=pred) 

manual_2sls <- lm(data=dataset, 
   formula = l1_logquantity ~ l1_logprice + l1_logincome + l1_logillegal + c_instrumented) 

stargazer(first_stage_reg, anderson_hsiao, manual_2sls, 
          label = "tab:reg", header=FALSE, model.names = FALSE,
          column.sep.width="-5pt",
          dep.var.labels=c("$C_{it-1} - C_{it-2}$", 
                           "$C_{it} - C_{it-1}$",
                           "$C_{it} - C_{it-1}$"),
          column.labels = c("First-Stage", "A-H", "Manual 2SLS"),
              omit.stat = c("ll", "ser", "rsq"))
```

The table is displayed below. The estimates from models (2) and (3) in tabel \ref{tab:reg} are the same. Only the variance of the 2SLS-estimator is off. Nevertheless, the results show a point estimate that is comparable in magnitude, and both of the point estimates are significantly different from zero. The assumptions underlying the approach are (i) no autocorrelation in the error terms, implying that the autoregressive order is correctly specified, and (ii) weak exogeneity, implying that contemporaneous error terms are unrelated to past values. Thirdly, and a less strict assumption is instrument relevance: the lagged level-endogenous variable should be a relevant instrument, meaning with sufficient power to predict the (contemporaneous) first-differences $Y_{it} - Y_{it-1}$. 

The interpretation of the estimates is in terms of elasticities. For example, a percent increase in the price of opium is associated with a 2,2% increase in consumption between two periods. This result is however not significant at commonly used significance levels. An interesting observation that can be made is the strong positive effect of income changes on opium consumption. This effect is significant at the 5% level for model (2) and significant at the 1% level for model (3). However, the standard error for model (3) is likely to be off. A final observation that can be made is the strong persistence we find for the 2 period lag of opium consumption, denoted by l2_logquantity. This effect is found to be significant at the 10% level by using the package and at the 1% level using the manual 2SLS. The estimate of l2_logquantity tells us that a percent increase in opium consumption in the past is associated with a 150% increase in the consumption of opium a period later in the future. This type of behaviour is likely to be valid for an addictive substance as opium.

<!---
Walter: misschien moeten we SEs clusteren per groep?
-->
\clearpage

4. Describe the Arellano & Bond GMM estimator for this model.

In general the Arellano & Bond GMM estimator aims to use lagged values of endogenous regressors as an instrument. All possible moment conditions are for $t=2,\dots,T$ and $k=2,\dots,t$, applied to the current model given by:

\begin{equation*}
\mathbb{E}[\log(C_{it-k})(u_{it}-u_{it-1})]
\end{equation*}

When there are more moment conditions than identifiable parameters, the A&B estimator has to deal with a overidentification. It does so by employing a weighing matrix of the moment conditions. The ideal weighing matrix minimizes the variance, and is usually obtained by generating a sensible estimate (a first-stage estimate with homoskedastic errors implies a Toeplitz-band structure with 2 on the diagonals, 1 on the off-diagonals), which is then used to estimate the parameters, which are then used to derive new estimates and a new matrix. This two-step procedure should lead to efficient standard errors. 

5. Estimate the model parameters using the Arellano & Bond estimator, tabulate the results and discuss the parameter estimates.

```{r, warning=FALSE}
# Here, we use the _normal_ specification as default and not the first difference:
# The package will transform the data accordingly
arellano_bond <- pgmm(data = dataset2,
                          logquantity ~ logincome + logprice + 
                            logillegal + lag(logquantity, 1) + as.numeric(year)
                          | lag(logquantity, 2:99), transformation = 'd', effect = 'individual')

arellano_bond2 <- pgmm(data = dataset2,
                          logquantity ~ logincome + logprice + 
                            logillegal + lag(logquantity, 1) + as.numeric(year)
                          | lag(logquantity, 2:5), transformation = 'd', effect = 'individual')

```

6. What is in your estimate for the short-run and the long-run price elasticity of opium?

Because the regression equation is in logs, the short-term price elasticity is simply the coefficient belonging to $\log \text{Price}$, which is `r arellano_bond$coefficients[2]`, meaning that a price increase of 1% means a consumption decrease of 0.42%, which is not small. The long-run price elasticity can be found by calculating the long-run multiplier of that coefficient: $LRM = \frac{\beta_{\text{log(Price)}}}{1 - \beta_{\text{log(Consumption)}_{it-1}}}$. Concretely, this means that the long-run multiplier in the model using the Arellano-Bond estimator is `r arellano_bond$coefficients[2]/(1-arellano_bond$coefficients[4])`. 


7. Now estimate the model parameters using the system estimator (Blundell & Bond). Tabulate results, compute the elasticities (as in 6.).

```{r results='asis', warning = FALSE}
blundell_bond <- pgmm(data = dataset2,
                          logquantity ~ logincome + logprice + 
                            logillegal + lag(logquantity, 1) + as.numeric(year)| 
                            lag(logquantity, 2:99) + lag(l1_logquantity, 2:99), transformation = 'ld', 
                            effect = 'individual')

blundell_bond2 <- pgmm(data = dataset2,
                          logquantity ~ logincome + logprice + 
                            logillegal + lag(logquantity, 1) + as.numeric(year)| 
                            lag(logquantity, 2:5) + lag(l1_logquantity, 2:5), transformation = 'ld', 
                            effect = 'individual')

no_obs <- list(arellano_bond, arellano_bond2, blundell_bond, blundell_bond2) %>%
  map(~ summary(.x)) %>%
  map_dbl(~ .x$fitted.values %>%
        length())
  
stargazer(arellano_bond, arellano_bond2, blundell_bond, blundell_bond2,
          label = "tab:reg_bb", header=FALSE, model.names = FALSE,
          column.sep.width="-5pt",
          add.lines=list(c("Observations", no_obs)),
           omit.stat = c("ll", "ser", "n"),
          column.labels = c("Arellano-Bond", "Arellano-Bond", "Blundell-Bond", "Blundell-Bond"))
```

The short-term elasticity is -0.546 for the model in which we used all lags possible (model (3)). For robustness, we used a smaller amount of instruments and find a very comparable estimate of -0.569 (model (4)). Both these results are significant at the 1% level. The long-run elasticity for model (3) is `r blundell_bond$coefficients[2]/(1-blundell_bond$coefficients[4])`. Model (4) yields a similar long-run elasticity of `r blundell_bond2$coefficients[2]/(1-blundell_bond2$coefficients[4])`.

8. Which parameter estimates do you prefer? Explain why. Are there remaining problems with your preferred estimates?

```{r results='asis', warning = FALSE}
ab_sargantest <- sargan(arellano_bond)
bb_sargantest <- sargan(blundell_bond)
```

Conducting the Sargan test on both models, we observe that for both models the null hypothesis is not rejected (for the Arellano-Bond model, we have a statistic of `r ab_sargantest$statistic` and a p-value of `r ab_sargantest$p.value`, and for the Blundell-Bond estimator, we have a statistic of `r bb_sargantest$statistic` and a p-value of `r bb_sargantest$p.value` This means that for both models we find no evidence to conclude that the models are misspecified.

We prefer the Blundell-Bond model estimates. The reason is that the system estimator is robust for estimating coefficients of variables that are very persistent through time. Opium consumption is likely to be a variable of such type. 

We believe that the following problems remain. First, we believe that our estimates are not causally interpretable. This is because there could be variables that are correlated with opium consumption which are not captured in the model. If there would be a source of randomization this could possibly be overcome. Second, our estimates seem to be not very robust to our model choice, hence the model choice matters a lot, and this could indicate underlying problems.









